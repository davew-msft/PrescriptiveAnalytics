{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://learning.oreilly.com/scenarios/define-and-understand/9781098121587/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this set is to introduce you to fundamental concepts of reinforcement learning (RL) in the context of recommendation systems.\n",
    "\n",
    "Typical applications include:\n",
    "\n",
    "Product recommendation in online shops\n",
    "Content recommendation on news websites\n",
    "Next best offer in personalized email campaigns\n",
    "\n",
    "\n",
    "Learning Goals\n",
    "Understand the core concepts of RL systems and how to apply them\n",
    "Understand how RL environments work and how to interact with them\n",
    "Get familiar with Google RecSim for easy environment development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Is an Environment?\n",
    "In an RL scenario, the computer (our agent) has to make a choice (action) from a possible set of action items (candidate documents).\n",
    "\n",
    "Depending on which documents the agent recommends, the user will take an action and send a response back to the agent. This response might include a reward, such as the play time of a video or the view time of a news article.\n",
    "\n",
    "All of these components form a so-called environment for our RL system, which is an abstraction of the world our agent (which can be powered by an RL algorithm) operates within.\n",
    "\n",
    "In practice, these environments are very complex, and the real user behavior is never known and can only be abstracted using a model.\n",
    "\n",
    "Instead of building such an environment from scratch, we will use the Python package RecSim from Google.\n",
    "\n",
    "RecSim allows us to define the different building blocks of a recommender system in an easy way and comes with pre-built environments.\n",
    "\n",
    "You can install RecSim with the following pip command:\n",
    "\n",
    "pip install recsim\n",
    "As you see from the \"Requirement already satisfied\" messages, RecSim is already installed on this machine.\n",
    "\n",
    "Clear the terminal output, run Python, and load the RecSim library with the following code:\n",
    "\n",
    "clear\n",
    "python\n",
    "import recsim\n",
    "from recsim import environments\n",
    "from pprint import pprint # for better print formatting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Is RecSim?\n",
    "The RecSim documentation on GitHub provides a figure that shows the different classes needed to build an environment in RecSim. The blue classes refer to the documents, the green classes to the user, and the red classes to the agent. The various boxes represent conditional probability distributions. Therefore, a RecSim environment can be seen as a dynamic Bayesian network (DBN).\n",
    "\n",
    "We won't cover all of them in detail. Instead, we will rely on pre-built environments, with some customizations.\n",
    "\n",
    "One customization is to define how many candidate documents should be observed by the agent (NUM_CANDIDATES) and how many of those should be recommended (SLATE_SIZE).\n",
    "\n",
    "Run the following code to define that our agent will get 10 candidate documents of which he has to recommend 3:\n",
    "\n",
    "NUM_CANDIDATES = 10\n",
    "SLATE_SIZE = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Environment\n",
    "RecSim comes with three pre-built environments:\n",
    "\n",
    "Long term satisfaction (the one we will use in this lab)\n",
    "Interest evolution\n",
    "Interest exploration\n",
    "Each of these environments makes different assumptions about the user behavior.\n",
    "\n",
    "We will continue with the long term satisfaction (LTS) environment. If you are interested in the other ones, check out the resource links at the end of this lab.\n",
    "\n",
    "The LTS environment simulates a situation where a user of an online service interacts with items of content, which are characterized by their level of clickbaitiness (on a scale of 0 to 1).\n",
    "\n",
    "Clickbaity items (choc) generate engagement but lead to a decrease in long-term satisfaction.\n",
    "\n",
    "Nonclickbaity items (kale) increase satisfaction but do not generate as much engagement.\n",
    "\n",
    "The challenge is to balance the two in order to achieve some long-term optimal trade-off, a very common scenario for content recommendation systems.\n",
    "\n",
    "Let's initialize the environment using the following code and plug in our previously defined NUM_CANDIDATES and SLATE_SIZE:\n",
    "\n",
    "lts_env = recsim.environments.long_term_satisfaction.create_environment({\n",
    "    \"num_candidates\": NUM_CANDIDATES,\n",
    "    \"slate_size\": SLATE_SIZE,\n",
    "    \"resample_documents\": False\n",
    "})\n",
    "We will come back to what resample_documents means in a bit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation Space\n",
    "Let's find out how our fresh environment looks.\n",
    "\n",
    "Run the following code to reset the environment to an initial state and print the initial observation:\n",
    "\n",
    "observation_space = lts_env.reset()\n",
    "pprint(observation_space)\n",
    "Take a look at the output.\n",
    "\n",
    "That's what an agent within our environment would see.\n",
    "\n",
    "You can see that there are ten documents available for the agent to choose from. Each document has one feature—the clickbaitiness (on a scale of 0 to 1).\n",
    "\n",
    "We can't see a response because it is still the initial state.\n",
    "\n",
    "We can see a user, but without any features describing the user.\n",
    "\n",
    "Let's reset the environment by running the code again.\n",
    "\n",
    "Try running the following code multiple times:\n",
    "\n",
    "observation_space = lts_env.reset()\n",
    "pprint(observation_space)\n",
    "You will see that the documents do not change; it will always be the same list of ten documents. That's because we set resample_documents to False when we set up the environment. That means the agent will always see the same documents for this environment.\n",
    "\n",
    "Let's change the parameter and allow our environment to resample documents by running the following code:\n",
    "\n",
    "lts_env_resample = recsim.environments.long_term_satisfaction.create_environment({\n",
    "    \"num_candidates\": NUM_CANDIDATES,\n",
    "    \"slate_size\": SLATE_SIZE,\n",
    "    \"resample_documents\": True\n",
    "})\n",
    "Run the following code multiple times:\n",
    "\n",
    "observation_space = lts_env_resample.reset()\n",
    "pprint(observation_space)\n",
    "You can see that the documents are now changing every time you run the code. That's because every time we observe a new state of the environment, we get a fresh sample of documents from the overall document store.\n",
    "\n",
    "In RecSim, you can't manually define documents; they always need to be sampled from a larger repository.\n",
    "\n",
    "To learn more about documents and how to set up your own documents model, check out the link in the resources at the end of this lab.\n",
    "\n",
    "So far, we have reset the environment a couple of times, but we haven't actually taken action in it. Let's do that now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Action Space\n",
    "As you can see from the previous outputs, the response variable and the user features did not change as we ran our code. That's because we didn't actually take action inside our environment but just initialized the environment over and over again.\n",
    "\n",
    "What actions can we take? Let's find out by exploring the action space.\n",
    "\n",
    "Run the following code:\n",
    "\n",
    "print(lts_env.action_space)\n",
    "As you can see from the output, our agent faces a MultiDiscrete action space.\n",
    "\n",
    "If you remember, we defined our environment in such a way that the agent can choose from 10 documents for 3 different slots (slates).\n",
    "\n",
    "And that is exactly what we can observe here—10 possible actions for each of the 3 slots.\n",
    "\n",
    "Let's interact with our environment manually, pretending we are the agent that tries to make the best recommendation to create long-term user satisfaction.\n",
    "\n",
    "Let's say we will always recommend the first three items from the document candidates.\n",
    "\n",
    "RECOMMENDATIONS = [0, 1, 2]\n",
    "Let's plug this into our environment and see what happens.\n",
    "\n",
    "We can apply an action using the step method.\n",
    "\n",
    "Run the following code:\n",
    "\n",
    "observation_0 = lts_env.reset()\n",
    "observation_1, reward, done, _ = lts_env.step(RECOMMENDATIONS)\n",
    "pprint(observation_1)\n",
    "You can see that this observation provides more detail than the ones before—it actually contains feedback from the user. Let's find out what that means."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Behavior\n",
    "The step method that we just called on the environment for returned a tuple of four items (observation, reward, done, info), where:\n",
    "\n",
    "observation is the agent's observation that includes user's state features, documents, user responses.\n",
    "reward is the amount of reward returned after the previous action.\n",
    "done is a boolean whether the episode has ended (time budget of the user is empty).\n",
    "info is a dictionary containing information for debugging/learning.\n",
    "In our case, the reward is the engagement that the user has shown for the documents that we suggested to him.\n",
    "\n",
    "In a real scenario, this reward could be the view time, for example.\n",
    "\n",
    "You should see from the output which of our suggestions was clicked by the user ('click': 1) and the engagement for it.\n",
    "\n",
    "Run this code to step through the environment until the user's time budget has been depleted, which means the end of our current episode:\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "lts_env.reset()\n",
    "while not done:\n",
    "  observation, reward, done, _ = lts_env.step(RECOMMENDATIONS)\n",
    "  total_reward += reward\n",
    "  step_count += 1\n",
    "\n",
    "print(\"Episode has ended after %(a)s steps with an accumulated reward of %(r)d. The latest observation was:\" % {'a': step_count, 'r': total_reward})\n",
    "As you can see, always selecting the first three documents has yielded an accumulated reward as printed in the output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Simulated User Interactions\n",
    "\n",
    "Import Packages\n",
    "As shown in the previous lab Define and Understand a Reinforcement Learning Environment, we will use an environment for a recommendation scenario using Google RecSim.\n",
    "\n",
    "You can install RecSim with pip install recsim, which is already done on this machine.\n",
    "\n",
    "Click on the Copy to Editor button to create a file interactions.py and add the necessary import statements for some utility packages of this scenario.\n",
    "\n",
    "Copy to Editor# Imports\n",
    "from pprint import pprint # for better print formatting\n",
    "import numpy as np\n",
    "import gym \n",
    "gym is a popular Python library for developing and comparing reinforcement learning algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Environment\n",
    "To load the long term satisfaction (LTS) environment from RecSim, add the following code to interactions.py:\n",
    "\n",
    "Copy to Editorfrom recsim.environments.long_term_satisfaction import create_environment as LongTermSatisfactionRecSimEnv\n",
    "\n",
    "Add the following code to create an environment with a slate_size of 3 and 20 document candidates:\n",
    "\n",
    "Copy to Editor# Environment\n",
    "lts_20_3 = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 20, \n",
    "    \"slate_size\": 3,\n",
    "    \"resample_documents\": True,\n",
    "    \"convert_to_discrete_action_space\": True\n",
    "})\n",
    "\n",
    "print(lts_20_3.action_space)\n",
    "\n",
    "Run python interactions.py to take a look at the action space.\n",
    "\n",
    "As you can see from the output MultiDiscrete([20 20 20], our action space lets us select 1 out of 20 candidate documents for 3 different slots.\n",
    "\n",
    "This already gives us a complexity of 20 * 20 * 20 = 8000 unique possible combinations.\n",
    "\n",
    "These kinds of recommendation problems will scale really fast in complexity!\n",
    "\n",
    "To keep things simple and computation time short, let's pretend we are dealing with only 100 candidate documents for 1 slot.\n",
    "\n",
    "To create the environment lts_100_1, add the following code:\n",
    "\n",
    "Copy to Editorlts_100_1 = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 100, \n",
    "    \"slate_size\": 1,\n",
    "    \"resample_documents\": True\n",
    "})\n",
    "print(lts_100_1.action_space)\n",
    "\n",
    "Run python interactions.py again.\n",
    "\n",
    "As you can see from the output MultiDiscrete([100]), we are now dealing with an environment with 100 possible documents. This is much less complex than the 8,000 possible combinations before and makes the calculations much easier for selecting the best documents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customize the Environment\n",
    "Remember, the LTS environment simulates a situation where a user of an online service interacts with items of content, which are characterized by their level of clickbaitiness (on a scale of 0 to 1).\n",
    "\n",
    "Clickbaity items (choc) generate engagement but lead to a decrease in long-term satisfaction.\n",
    "\n",
    "Nonclickbaity items (kale) increase satisfaction but do not generate as much engagement.\n",
    "\n",
    "The challenge is to balance the two in order to achieve some long-term optimal trade-off, a very common scenario for content recommendation systems.\n",
    "\n",
    "This effect is, per default, very small.\n",
    "\n",
    "Inspect the settings of the user model in a RecSim environment using the following code:\n",
    "\n",
    "Copy to Editorpprint(lts_100_1.environment._user_model._user_sampler._state_parameters)\n",
    "\n",
    "Run python interactions.py to take a look.\n",
    "\n",
    "We will now update some of these parameters to make the satisfaction effect even larger for demonstration purposes (or we assume that our users are even less likely to become disengaged by too many clickbaity recommendations).\n",
    "\n",
    "Run the following code to update the sensitivity, time_budget, choc_stddev, and kale_stddev in the user model:\n",
    "\n",
    "Copy to Editorlts_100_1.environment._user_model._user_sampler._state_parameters.update({\n",
    "            \"sensitivity\": 0.1,\n",
    "            \"time_budget\": 200,\n",
    "            \"choc_stddev\": 0.01,\n",
    "            \"kale_stddev\": 0.01\n",
    "        })\n",
    "\n",
    "pprint(lts_100_1.environment._user_model._user_sampler._state_parameters)\n",
    "\n",
    "Inspect the updated settings by running python interactions.py."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Random Actions\n",
    "At first, we want to explore how to interact with our environment using an agent that performs random actions.\n",
    "\n",
    "Use the following code to define a function that steps through the environment until the user's time budget is depleted. We call this process one episode. The agent collects the rewards based on random recommendations.\n",
    "\n",
    "Add the following code to define the function that runs one episode and call that function for the environment lts_100_1:\n",
    "\n",
    "Copy to Editordef run_one_episode_random(env, verbose = False):\n",
    "  # Runs one episode until user time budget is depleted.\n",
    "  # Returns all rewards in a list.\n",
    "\n",
    "  # Sum of rewards for one episode\n",
    "  episode_rewards = []\n",
    "\n",
    "  # Step counter\n",
    "  step = 0\n",
    "\n",
    "  # Reset environment before each episode\n",
    "  env.reset()\n",
    "\n",
    "  # User time budget\n",
    "  done = False\n",
    "\n",
    "  # Run episode until user time budget is depleted\n",
    "  while not done:\n",
    "    # Pick random action\n",
    "    action = env.action_space.sample()\n",
    "    # Perform action   \n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    # Gather reward\n",
    "    episode_rewards.append(reward)\n",
    "    # Count step\n",
    "    step += 1\n",
    "\n",
    "    if verbose:\n",
    "      print(\"Step %(s)d completed with a reward of %(r)d.\" % {'s': step, 'r': reward})\n",
    "\n",
    "  return(episode_rewards)\n",
    "\n",
    "random_rewards = run_one_episode_random(lts_100_1, verbose = True)\n",
    "\n",
    "Run python interactions.py to observe the rewards.\n",
    "\n",
    "As you can see from the output, the reward after each step (the user engagement—e.g., the watch time of a video) goes up and down without any real pattern.\n",
    "\n",
    "We can also visualize these rewards using a simple plot. The plot will show each reward as a point and draws a trendline over time. Add the following code to create the plot random-rewards.png:\n",
    "\n",
    "Copy to Editorimport matplotlib.pyplot as plt\n",
    "#create scatterplot\n",
    "y = random_rewards\n",
    "x = range(0,len(y))\n",
    "plt.scatter(x, y, c=\"lightgrey\")\n",
    "\n",
    "#calculate equation for trendline\n",
    "z = np.polyfit(x, y, 1)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "#add trendline to plot\n",
    "plt.plot(x, p(x))\n",
    "plt.savefig(\"random-rewards.png\")\n",
    "Run python interactions.py again and open random-rewards.png.\n",
    "\n",
    "You should see a relatively steady trendline.\n",
    "\n",
    "Run python interactions.py again and open random-rewards.png.\n",
    "\n",
    "If you do this a couple times, you should see that the trend line sometimes goes up and sometimes goes down—an indication of the random effect here.\n",
    "\n",
    "Our ultimate goal is to create an agent where the rewards get bigger over time—i.e., a trend line that goes mostly up instead of being flat."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Greedy Actions\n",
    "Let's run the same experiment using a greedy agent. That means our algorithm should always pick the most clickbaity item. What will happen? Let's find out!\n",
    "\n",
    "The following code works just as the run_one_episode_random function; the only difference is that np.argmax([value for _, value in obs[\"doc\"].items()]) will select the highest clickbaity item.\n",
    "\n",
    "Copy to Editordef run_one_episode_greedy(env, verbose = False):\n",
    "  # Runs one episode until user time budget is depleted.\n",
    "  # Returns all rewards in a list.\n",
    "\n",
    "  # Sum of rewards for one episode\n",
    "  episode_rewards = []\n",
    "\n",
    "  # Step counter\n",
    "  step = 0\n",
    "\n",
    "  # Reset environment after each episode\n",
    "  obs = env.reset()\n",
    "  done = False\n",
    "\n",
    "  # Run episode until user time budget is depleted\n",
    "  while not done:\n",
    "    # Pick most clickbaity item\n",
    "    sweetest = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    # Perform action   \n",
    "    obs, reward, done, _ = env.step([sweetest])\n",
    "    # Gather reward\n",
    "    episode_rewards.append(reward)\n",
    "    # Count step\n",
    "    step += 1\n",
    "\n",
    "    if verbose:\n",
    "      print(\"Step %(s)d completed with a reward of %(r)d.\" % {'s': step, 'r': reward})\n",
    "\n",
    "  return(episode_rewards)\n",
    "\n",
    "greedy_rewards = run_one_episode_greedy(lts_100_1, verbose = False)\n",
    "\n",
    "We will also create a plot for the greedy behavior that is stored under greedy-rewards.png.\n",
    "\n",
    "Add the following code to the file:\n",
    "\n",
    "Copy to Editory = greedy_rewards\n",
    "x = range(0,len(y))\n",
    "plt.clf() # clear plot\n",
    "plt.scatter(x, y, c=\"lightgrey\")\n",
    "\n",
    "#calculate equation for trendline\n",
    "z = np.polyfit(x, y, 1)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "#add trendline to plot\n",
    "plt.plot(x, p(x))\n",
    "plt.savefig(\"greedy-rewards.png\")\n",
    "Run python interactions.py again and open greedy-rewards.png.\n",
    "\n",
    "You should see that the trendline always points down and the rewards get smaller over time.\n",
    "\n",
    "This shows that the user satisfaction goes down over time when they only consume clickbaity items."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate a Random Baseline\n",
    "Finally, before we continue to create more sophisticated agents using reinforcement learning, we want to find out which baseline these agents actually need to beat.\n",
    "\n",
    "For this purpose, we will take over 1,000 random episodes to get a reliable mean of the average total reward values.\n",
    "\n",
    "The following formula takes the run_one_episode_random function and executes it within a while loop that runs for a number of 1,000 episodes.\n",
    "\n",
    "The result of this loop will be the mean of all episode rewards, which is our random_baseline for this environment.\n",
    "\n",
    "Add the following code to create the function and call over 500 episodes:\n",
    "\n",
    "Copy to Editordef get_random_baseline_for_env(env, episodes=500, verbose=False):\n",
    "\n",
    "  # counts number of elapsed episodes\n",
    "  episodes_count = 0\n",
    "  # Lists all sum of rewards over all episodes\n",
    "  episodes_all_rewards = [] \n",
    "\n",
    "  while episodes_count < episodes:\n",
    "    episode_rewards = run_one_episode_random(env, verbose = False)\n",
    "    episode_reward_sum = np.sum(episode_rewards)\n",
    "\n",
    "    if verbose:\n",
    "      print(\"Episode %(e)d elapsed with accumulated reward of %(r)d.\" % {'e': episodes_count, 'r': episode_reward_sum})\n",
    "    elif episodes_count % 100 == 0:\n",
    "      print(f\" {episodes_count} \", end=\"\")\n",
    "    elif episodes_count % 10 == 0:\n",
    "      print(\".\", end=\"\")\n",
    "\n",
    "    # Count one episode\n",
    "    episodes_count += 1\n",
    "    episodes_all_rewards.append(episode_reward_sum) \n",
    "\n",
    "  random_baseline = np.mean(episodes_all_rewards)\n",
    "\n",
    "  print(\"%(E)d Episodes ended with a total accumulated reward of %(r)d.\" % {'E': episodes, 'r': random_baseline})\n",
    "  return(random_baseline)\n",
    "\n",
    "random_baseline = get_random_baseline_for_env(lts_100_1, verbose = False)\n",
    "print(random_baseline)\n",
    "\n",
    "Run python interactions.py, which might take a couple of minutes to complete.\n",
    "\n",
    "Inspect the output to see which reward is shown for the random baseline.\n",
    "\n",
    "Now, whatever we do with reinforcement learning or any other technique, it must be able to beat at least this baseline if we are operating in this environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Contextual Bandit Using RLLib\n",
    "\n",
    "What Is RLlib?\n",
    "RecSim provides us with a library that gives us easy access to customizable RL environments for recommendation scenarios.\n",
    "\n",
    "To interact with an environment, we can code up primitive agents ourselves, simulating random or greedy behavior, for example (see the lab Set Up Simulated User Interactions).\n",
    "\n",
    "There are different ways and technologies to build more complex agents and train them using RL.\n",
    "\n",
    "One popular way is to use a Python library called RLlib.\n",
    "\n",
    "RLlib is a native library of the Ray project—an open-source software developed at UC Berkeley that allows compute-intensive Python workloads, including deep RL.\n",
    "\n",
    "RLlib offers support for production-level RL workloads while providing simple APIs for very different kinds of applications.\n",
    "\n",
    "You can install RLlib using the command pip install \"ray[rllib]\" (already installed on this machine).\n",
    "\n",
    "In addition to RLlib, we also need RecSim for the environment and PyTorchas the deep learning backend; you can install both using the following pip command: pip install recsim torch. Again, no need to run it here as both are already preinstalled.\n",
    "\n",
    "Click on the following Copy to Editor button to create a file train.py and add the necessary import statements for this lab:\n",
    "\n",
    "Copy to Editor# Imports\n",
    "import ray\n",
    "import gym\n",
    "import numpy as np\n",
    "from ray import tune\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import progressbar\n",
    "from os.path import exists\n",
    "ray.init(object_store_memory=78643200) # Limit for this machine, delete when run on your own system"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Environment\n",
    "RLlib comes with RecSim environments included so we can import them directly from there.\n",
    "\n",
    "Add the following code to import the LongTermSatisfactionRecSimEnv:\n",
    "\n",
    "Copy to Editor# Environment\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n",
    "\n",
    "Again, we will make some customizations to make the satisfaction effect even stronger. But this time we are going to write a small wrapper function, which makes it a bit more flexible for us to pass these config parameters to a RecSim environment.\n",
    "\n",
    "Add the following code to define the StrongLTS wrapper function:\n",
    "\n",
    "Copy to Editordef StrongLTS(env):\n",
    "    env.environment._user_model._user_sampler._state_parameters.update({\n",
    "            \"sensitivity\": 0.06,\n",
    "            \"time_budget\": 120,\n",
    "            \"choc_stddev\": 0.1,\n",
    "            \"kale_stddev\": 0.1\n",
    "        })\n",
    "    return(env)\n",
    "\n",
    "Then add the following code to create the environment strong_lts_20_2 using the wrapper:\n",
    "\n",
    "Copy to Editorlts_20_2 = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 20, \n",
    "    \"slate_size\": 2,\n",
    "    \"resample_documents\": True,\n",
    "    \"convert_to_discrete_action_space\": True\n",
    "})\n",
    "\n",
    "strong_lts_20_2 = StrongLTS(lts_20_2)\n",
    "\n",
    "This will create an environment with 2 slates and 20 candidate documents, giving a possible 20 * 20 = 400 unique choices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Random Baseline\n",
    "Open the file random_baseline.py.\n",
    "\n",
    "This file contains essentially the same functionality we have seen in the lab Set Up Simulated User Interactions.\n",
    "\n",
    "The function get_random_baseline_for_env runs a custom number of episodes and returns the accumulated reward baseline for an agent acting with random behavior.\n",
    "\n",
    "To access this function, add the following two lines to the script train.py:\n",
    "\n",
    "Copy to Editor# Calculate random baseline\n",
    "from random_baseline import get_random_baseline_for_env\n",
    "random_baseline = get_random_baseline_for_env(strong_lts_20_2, verbose = False)\n",
    "\n",
    "Now run train.py to calculate the random baseline over 500 episodes (note that this can take a few minutes):\n",
    "\n",
    "python train.py\n",
    "The baseline should be around 1157. For convenience this value has been stored in a file called random_baseline.txt. You can always access it from there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Trainers in RLlib\n",
    "Let's go beyond the random agent.\n",
    "\n",
    "Comment out the random_baseline calculation by clicking the Copy to Editor button below:\n",
    "\n",
    "Copy to Editor# random_baseline = get_random_baseline_for_env(strong_lts_20_2, verbose = False)\n",
    "RLlib comes with a large amount of prebuilt algorithms (trainers). You can find a complete list of these algorithms in the resources at the end.\n",
    "\n",
    "In order to use one of these algorithms, you have to instantiate its associated trainer class.\n",
    "\n",
    "Add the following code to import a multi-armed bandit trainer with Upper Confidence Bound (UCB) exploration:\n",
    "\n",
    "Copy to Editor# Trainer\n",
    "from ray.rllib.agents.bandit import BanditLinUCBTrainer\n",
    "from ray.rllib.agents.bandit.bandit import DEFAULT_CONFIG as BANDIT_DEFAULT_CONFIG\n",
    "pprint(BANDIT_DEFAULT_CONFIG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure a Trainer\n",
    "Run python train.py and inspect the output.\n",
    "\n",
    "What you see here are the default configurations for this trainer class.\n",
    "\n",
    "As you can see, there are plenty of ways to customize trainers.\n",
    "\n",
    "You can configure trainers in RLlib with a configuration dictionary.\n",
    "\n",
    "Let's update some of these configs for our own needs.\n",
    "\n",
    "For example, we want to specify the name of the environment and the environment setup.\n",
    "\n",
    "Add the following with the custom config dictionary:\n",
    "\n",
    "Copy to Editorbandit_config = {\n",
    "    \"env\": \"strong_lts\",\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 20, \n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "\n",
    "        # Bandit-specific flags:\n",
    "        \"convert_to_discrete_action_space\": True,\n",
    "        # Convert \"doc\" key into \"item\" key.\n",
    "        \"wrap_for_bandits\": True,\n",
    "        # Set consistent random seeds for the environment\n",
    "        \"seed\": 123,\n",
    "    },\n",
    "    # Seed for the trainer.\n",
    "    \"seed\": 123\n",
    "}\n",
    "\n",
    "As you can see, we pass our environment here as a string value strong_lts.\n",
    "\n",
    "The most convenient way to hook up a trainer from RLlib with a custom gym environment is to register the environment in Ray.\n",
    "\n",
    "Add the following code to register our environment:\n",
    "\n",
    "Copy to Editor# Register the environment \n",
    "tune.register_env(\"strong_lts\", lambda env_config: StrongLTS(LongTermSatisfactionRecSimEnv(env_config)))\n",
    "\n",
    "To actually create the trainer object with the above config, add the following code:\n",
    "\n",
    "Copy to Editorbandit_trainer = BanditLinUCBTrainer(config=bandit_config)\n",
    "\n",
    "Finally, add the following code to inspect our trainer briefly:\n",
    "\n",
    "Copy to Editorpprint(bandit_trainer.get_config())\n",
    "print(bandit_trainer.iteration)\n",
    "\n",
    "Run python train.py to take a look.\n",
    "\n",
    "You will see our trainer's configuration: for example, the env used and the expected action_space associated with it.\n",
    "\n",
    "Also, our trainer is currently at iteration 0, which means this trainer has not been trained yet!\n",
    "\n",
    "Let's change that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a Single Training\n",
    "It's time to run the first training!\n",
    "\n",
    "One training iteration usually includes the following two steps:\n",
    "\n",
    "Perform an action in the environment\n",
    "Use the observed data (observations, actions taken, rewards) to update the policy model (e.g., a neural network) such that it would pick better actions in the future, leading to higher rewards.\n",
    "Trainers in RLlib are trained using the .train method.\n",
    "\n",
    "Add the following code to perform a single training call and print the results:\n",
    "\n",
    "Copy to Editorresult = bandit_trainer.train()\n",
    "del result[\"config\"] # Erase config for better readability.\n",
    "pprint(result)\n",
    "\n",
    "Run python train.py and inspect the output.\n",
    "\n",
    "You should see that our trainer is now at iteration 1.\n",
    "\n",
    "So far, the trainer has not really learned anything but we can verify that the training is technically working correctly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a Training Loop\n",
    "We need to train our trainer many more times in order to find the best policy (model) that recommends the best actions for our environment.\n",
    "\n",
    "In an RL setting, this can easily cover a couple of thousand episodes.\n",
    "\n",
    "There are different learning approaches. In our case, we perform online learning where the model will be updated every time we receive the reward.\n",
    "\n",
    "Add the following function, which performs a training over a given period of episodes. This function will also visualize the training progress and save it as a bandit-training.png file:\n",
    "\n",
    "Copy to Editordef run_training(trainer, episodes):\n",
    "  # Train for n episodes and collect rewards.\n",
    "  rewards = []\n",
    "  for i in progressbar.progressbar(range(episodes)):\n",
    "      # Update the model immediately on the received reward.\n",
    "      result = trainer.train()\n",
    "      # Collect rewards\n",
    "      rewards.append(result[\"episode_reward_mean\"])\n",
    "\n",
    "  # Free up resources\n",
    "  trainer.stop()\n",
    "\n",
    "  # Plot episode rewards\n",
    "  plt.figure(figsize=(10,7))\n",
    "  start_at = 0\n",
    "  smoothing_win = 200\n",
    "  x = list(range(start_at, len(rewards)))\n",
    "  y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) for i in range(start_at, len(rewards))]\n",
    "  plt.plot(x, y)\n",
    "  plt.title(\"Average reward\")\n",
    "  plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "  # Add average random baseline reward (red line).\n",
    "  with open (\"random_baseline.txt\", \"r\") as f:\n",
    "    random_baseline = int(f.read())\n",
    "  plt.axhline(y=random_baseline, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "  plt.savefig('bandit-training.png')\n",
    "\n",
    "To actually run the training over 100 epochs, let's also add the following function call:\n",
    "\n",
    "Copy to Editorrun_training(bandit_trainer, 100)\n",
    "\n",
    "Before we run the script, we want to actually make sure that the result of the training process is saved.\n",
    "\n",
    "Add the following lines to save the checkpoints and keep the latest checkpoint path in a separate file checkpoint.txt:\n",
    "\n",
    "Copy to Editorcheckpoint_path = bandit_trainer.save(\"checkpoints\")\n",
    "print(f\"Trainer was saved in '{checkpoint_path}' at iteration {bandit_trainer.iteration}.\")\n",
    "\n",
    "with open('checkpoint_path.txt', 'w') as f:\n",
    "    f.write(checkpoint_path)\n",
    "\n",
    "Now run the file python train.py.\n",
    "\n",
    "Wait until the training is finished, indicated by the message Trainer was saved in 'checkpoints/checkpoint_...' at iteration 101."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Training Performance\n",
    "Let's find out how our trainer actually performs.\n",
    "\n",
    "Open bandit-training.png and inspect the result.\n",
    "\n",
    "You should see that the trainer performance actually drops after the first couple of iterations.\n",
    "\n",
    "At some point, however, the trainer picks up a better policy and successively gets closer to the random baseline (red horizontal line).\n",
    "\n",
    "If you like, you can rerun this training process with more than 100 steps and see how far the total reward gets.\n",
    "\n",
    "At some point, the trainer will hit a plateau very close to the baseline. Why is that? Let's find out by inspecting the trainer output!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer Inference—Inspect Outputs\n",
    "To find out what our trainer actually recommends, let's run some inference:\n",
    "\n",
    "Open the file inference.py.\n",
    "\n",
    "This file restores our trainer from the checkpoint and runs inference in a given environment.\n",
    "\n",
    "Run python inference.py and inspect the result.\n",
    "\n",
    "The output shows the observed documents and the action that the trainer suggests.\n",
    "\n",
    "If you look carefully, you will see that the bandit always chooses the document with the highest feature value, i.e., the most clickbaity item. You can run python inference.py a couple more times to back up our assumption.\n",
    "\n",
    "The reason for this behavior is that the bandit has learned to optimize for short-term rewards.\n",
    "\n",
    "And that's why it won't be able to capture the long-term goal of keeping the user satisfaction high as well.\n",
    "\n",
    "For that we will need a different algorithm that can handle a multigoal objective.\n",
    "\n",
    "We will try to improve this behavior using the SlateQ algorithm in the next lab Train a Deep Neural Network with SlateQ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Deep Neural Network with SlateQ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n",
    "Click the Copy to Editor button below to create a file train.py and import the following packages:\n",
    "\n",
    "Copy to Editor# Imports\n",
    "\n",
    "import ray\n",
    "import gym\n",
    "import numpy as np\n",
    "from ray import tune\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import progressbar\n",
    "from os.path import exists\n",
    "ray.init(object_store_memory=78643200)\n",
    "\n",
    "These packages allow us to set up our reinforcement learning environment and implement state-of-the-art deep learning algorithms without having to write too much boilerplate code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment\n",
    "Let's define our environment using RecSim via RLlib just like we did in the lab Build a Contextual Bandit Using RLlib.\n",
    "\n",
    "This time, however, we can't use a simple function to modify the environment parameters.\n",
    "\n",
    "The SlateQ algorithm requires an observation space with information (features) about the users.\n",
    "\n",
    "To add this information, we can define a short wrapper class.\n",
    "\n",
    "This wrapper class StrongLTS will:\n",
    "\n",
    "Increase the satisfaction effect in the user model as in the previous labs\n",
    "Modify the observation space so it includes information about the user's state\n",
    "Add the class by copying the following code:\n",
    "\n",
    "Copy to Editor# Environment\n",
    "\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n",
    "\n",
    "# LTS (Long Term Satisfaction) wrapper\n",
    "class StrongLTS(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Change user model\n",
    "        env.environment._user_model._user_sampler._state_parameters.update({\n",
    "            \"sensitivity\": 0.06,\n",
    "            \"time_budget\": 120,\n",
    "            \"choc_stddev\": 0.1,\n",
    "            \"kale_stddev\": 0.1\n",
    "        })\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Add user features to observation space\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            self.observation_space.spaces[\"user\"] = gym.spaces.Box(0.0, 1.0, (1, ), dtype=np.float32)\n",
    "            for r in self.observation_space[\"response\"]:\n",
    "                if \"engagement\" in r.spaces:\n",
    "                    r.spaces[\"watch_time\"] = r.spaces[\"engagement\"]\n",
    "                    del r.spaces[\"engagement\"]\n",
    "                    break\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            observation[\"user\"] = np.array([self.env.environment._user_model._user_state.satisfaction])\n",
    "            for r in observation[\"response\"]:\n",
    "                if \"engagement\" in r:\n",
    "                    r[\"watch_time\"] = r[\"engagement\"]\n",
    "                    del r[\"engagement\"]\n",
    "        return observation\n",
    "Finally, add the following line to register the environment in Ray:\n",
    "\n",
    "Copy to Editortune.register_env(\"strong_lts\", lambda env_config: StrongLTS(LongTermSatisfactionRecSimEnv(env_config)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the Trainer\n",
    "Thanks to the APIs in RLlib, setting up and configuring the SlateQ trainer works very similar to the bandit.\n",
    "\n",
    "Add the following code to import the trainer object from RLlib and pass the configuration parameters in a dictionary:\n",
    "\n",
    "Copy to Editorfrom ray.rllib.agents.slateq import SlateQTrainer\n",
    "\n",
    "slateq_config = {\n",
    "    \"env\": \"strong_lts\",\n",
    "    \"framework\": \"torch\",\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 20, \n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "slateq_trainer = SlateQTrainer(config=slateq_config)\n",
    "Let's go ahead and train the trainer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "Add the following training function, which we used in the lab Build a Contextual Bandit Using RLlib.\n",
    "\n",
    "This function will train over a specified number of episodes and visualize the training progress in a PNG file:\n",
    "\n",
    "Copy to Editor# Training\n",
    "\n",
    "def run_training(trainer, episodes):\n",
    "  rewards = []\n",
    "  for i in progressbar.progressbar(range(episodes)):\n",
    "      result = trainer.train()\n",
    "      rewards.append(result[\"episode_reward_mean\"])\n",
    "\n",
    "  trainer.stop()\n",
    "  plt.figure(figsize=(10,7))\n",
    "  start_at = 0\n",
    "  smoothing_win = 200\n",
    "  x = list(range(start_at, len(rewards)))\n",
    "  y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) for i in range(start_at, len(rewards))]\n",
    "  plt.plot(x, y)\n",
    "  plt.title(\"Average reward\")\n",
    "  plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "  # Add random baseline reward (red line).\n",
    "  with open (\"random_baseline.txt\", \"r\") as f:\n",
    "    random_baseline = int(f.read())\n",
    "  plt.axhline(y=random_baseline, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "  plt.savefig('slateq-training.png')\n",
    "\n",
    "Call the function for 5 episodes by adding the following code:\n",
    "\n",
    "Copy to Editor# Run Training\n",
    "run_training(slateq_trainer, 5)\n",
    "\n",
    "You will see that training the SlateQ model takes much longer than the bandit. It's much heavier and more complex, but it also has more powerful capabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Checkpoints\n",
    "Before we perform the training, add the following code to save the checkpoints as well as the checkpoint path to a separate file:\n",
    "\n",
    "Copy to Editor# Save checkpoints\n",
    "checkpoint_path = slateq_trainer.save(\"checkpoints\")\n",
    "print(f\"Trainer was saved in '{checkpoint_path}' at iteration {slateq_trainer.iteration}.\")\n",
    "\n",
    "with open('checkpoint_path.txt', 'w') as f:\n",
    "    f.write(checkpoint_path)\n",
    "\n",
    "Now run the training job:\n",
    "\n",
    "python train.py\n",
    "This will probably take 2-3 minutes to complete."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the Training Progress\n",
    "Once the training is finished, take a look at the chart slateq-training.png.\n",
    "\n",
    "Depending on the decisions made by the algorithm at the beginning, it starts either above or below the baseline.\n",
    "\n",
    "Also, the accumulated reward during the first few episodes might still be decreasing. This is because the algorithm is still in the exploration phase to find the best policy.\n",
    "\n",
    "So far, we have just trained over 5 training iterations, so that's fine.\n",
    "\n",
    "How can we improve the algorithm to achieve higher rewards? The easiest way is to train more!\n",
    "\n",
    "As long as you see that the average reward curve is still moving up or down, not horizontally, that's a good indication that the algorithm is still learning/exploring.\n",
    "\n",
    "To continue the training where we left off, add the following code before calling the training function to restore the trainer from the saved checkpoints:\n",
    "\n",
    "Copy to Editor# Run Training\n",
    "if exists(\"checkpoint_path.txt\"):\n",
    "  # Restore trainer\n",
    "  with open (\"checkpoint_path.txt\", \"r\") as f:\n",
    "    checkpoint_path = f.read()\n",
    "  slateq_trainer.restore(checkpoint_path)\n",
    "\n",
    "Now run the training script:\n",
    "\n",
    "python train.py\n",
    "Once the training is finished, check the chart slateq-training.png to inspect your training progress.\n",
    "\n",
    "You should see how the overall reward improves episode over episode. If not, try running the training for some more times.\n",
    "\n",
    "After around 40 episodes, the SlateQ algorithm should perform much better than the bandit and the baseline, hitting an average accumulated reward of more than 1160.\n",
    "\n",
    "Of course there are many more ways to improve the training performance. Another way is to tune the algorithm's hyper parameters using a grid search.\n",
    "\n",
    "While hyperparameter tuning would be out of the scope of this tutorial, you will find some useful links in the resources at the end."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the Policy\n",
    "Let's take a look under the hood of our trainer to understand the policy's model even better.\n",
    "\n",
    "First, be sure to comment out the run_training command since we don't want to run another training loop.\n",
    "\n",
    "You can comment out the line manually or click the Copy to Editor button below:\n",
    "\n",
    "Copy to Editor#run_training(slateq_trainer, 5)\n",
    "Now add the following code to show the trainer policy as well as the underlying model:\n",
    "\n",
    "Copy to Editorpolicy = slateq_trainer.get_policy()\n",
    "model = policy.model\n",
    "print(f\"Policy: {policy}\")\n",
    "print(f\"Policy's model: {model}\")\n",
    "Execute the training file again:\n",
    "\n",
    "python train.py\n",
    "As you can see from the output, the model is a sequential deep learning model, as you might recognize from learning about supervised machine learning.\n",
    "\n",
    "It contains inputs (the observations) and outputs (the actions).\n",
    "\n",
    "As with any deep learning model you can also train this model on historic data.\n",
    "\n",
    "In the context of reinforcement learning, this process is called offline learning, another powerful technique that can help you improve the quality of your model.\n",
    "\n",
    "Offline learning will also help you if you don't have a training environment but want to learn purely from historical data. If you want to find out more about offline learning, check out the resource links at the end of this lab."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
